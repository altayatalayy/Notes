<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Information Theory | Engineering Notes</title><meta name=keywords content><meta name=description content="The Origins of Information Theory A particular quantity called entropy is used in thermodynamics and in statistical mechanics. A quantity called entropy is used in communication theory. After all thermodynamics and statistical mechanics are older than communication theory. From these facts we might conclude that communication theory somehow grew out of statistical mechanics.
This easy but misleading idea has caused a great deal of confusion even among technical men. Actually, communication theory evolved from an effort to solve certain problems in the field of electrical communication."><meta name=author content><link rel=canonical href=https://altayatalayy.github.io/Notes/posts/information_theory/><link crossorigin=anonymous href=/Notes/assets/css/stylesheet.8c8e4ffe662331c9ef1da1e51c2adc5fb504213345cca4867ee8e4f36c0d7bd4.css integrity="sha256-jI5P/mYjMcnvHaHlHCrcX7UEITNFzKSGfujk82wNe9Q=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/Notes/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://altayatalayy.github.io/Notes/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://altayatalayy.github.io/Notes/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://altayatalayy.github.io/Notes/favicon-32x32.png><link rel=apple-touch-icon href=https://altayatalayy.github.io/Notes/apple-touch-icon.png><link rel=mask-icon href=https://altayatalayy.github.io/Notes/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/katex.min.css integrity=sha384-bYdxxUwYipFNohQlHt0bjN/LCpueqWz13HufFEV1SUatKs1cm4L6fFgCi1jT643X crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/katex.min.js integrity=sha384-Qsn9KnoKISj6dI8g7p1HBlNpVx0I8p1SvlwOldgi3IorMle61nQy4zEahWYtljaz crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous onload=renderMathInElement(document.body)></script><meta property="og:title" content="Information Theory"><meta property="og:description" content="The Origins of Information Theory A particular quantity called entropy is used in thermodynamics and in statistical mechanics. A quantity called entropy is used in communication theory. After all thermodynamics and statistical mechanics are older than communication theory. From these facts we might conclude that communication theory somehow grew out of statistical mechanics.
This easy but misleading idea has caused a great deal of confusion even among technical men. Actually, communication theory evolved from an effort to solve certain problems in the field of electrical communication."><meta property="og:type" content="article"><meta property="og:url" content="https://altayatalayy.github.io/Notes/posts/information_theory/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-09-02T22:13:31+03:00"><meta property="article:modified_time" content="2022-09-02T22:13:31+03:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Information Theory"><meta name=twitter:description content="The Origins of Information Theory A particular quantity called entropy is used in thermodynamics and in statistical mechanics. A quantity called entropy is used in communication theory. After all thermodynamics and statistical mechanics are older than communication theory. From these facts we might conclude that communication theory somehow grew out of statistical mechanics.
This easy but misleading idea has caused a great deal of confusion even among technical men. Actually, communication theory evolved from an effort to solve certain problems in the field of electrical communication."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://altayatalayy.github.io/Notes/posts/"},{"@type":"ListItem","position":2,"name":"Information Theory","item":"https://altayatalayy.github.io/Notes/posts/information_theory/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Information Theory","name":"Information Theory","description":"The Origins of Information Theory A particular quantity called entropy is used in thermodynamics and in statistical mechanics. A quantity called entropy is used in communication theory. After all thermodynamics and statistical mechanics are older than communication theory. From these facts we might conclude that communication theory somehow grew out of statistical mechanics.\nThis easy but misleading idea has caused a great deal of confusion even among technical men. Actually, communication theory evolved from an effort to solve certain problems in the field of electrical communication.","keywords":[],"articleBody":"The Origins of Information Theory A particular quantity called entropy is used in thermodynamics and in statistical mechanics. A quantity called entropy is used in communication theory. After all thermodynamics and statistical mechanics are older than communication theory. From these facts we might conclude that communication theory somehow grew out of statistical mechanics.\nThis easy but misleading idea has caused a great deal of confusion even among technical men. Actually, communication theory evolved from an effort to solve certain problems in the field of electrical communication. Its entropy was called by mathematical analogy with the entropy of statistical mechanics.\nWhile thermodynamics gave us the concept of entropy, it does not give a detailed picture of entropy, in terms of positions and velocities of molecules, for instance. Statistical mechanics does give a detailed mechanical meaning to entropy in particular cases. In general, the meaning is that an increase in entropy means a decrease in order. Order in the sense in which it is used in statistical mechanics involves unpredictability based on a lack of knowledge of the positions and velocities of molecules.\nIn communication theory we consider a message source, such as a writer or a speaker, which may produce on a given occasion any one of many possible messages. The amount of information conveyed by the message increases as the amount of uncertainty as to what message actually will be produced becomes greater. A message which is one out of ten messages conveys a smaller amount of information than a message which is one out of a million possible messages. The entropy of communication theory is a measure of this uncertainty and the uncertainty, or entropy is taken as the measure of the amount of information conveyed by a message source.\nIn single-current telegraphy we have two elements out of which to construct our code: current and no current, which we might call 1 and 0. In double-current telegraphy we really have three elements, which we might characterize as forward current; no current; backward current; or as +1,0,-1. In 1874 Thomas Edison went further; in his quadruplex telegraph system he used two intensities of current as well as two directions of current. How much information it is possible to send over a circuit depends not only on how fast one can send successive symbols over the circuit but also on how many different symbols one has avaible to choose among.\nThen use of multiplicity of symbols can lead to difficulties. Dots and dashes sent over a long cable tend to spread out and overlap. Thus, when we look for one symbol at the far end we see a little of several others.\nExtraneous currents, which we call noise are always present to interfere with signals sent. Thus, even if we avoid the overlapping of dots and spaces which is called symbol interference, noise tends to distort received signal.\nReferences [1] Pierce, J. R. (2012). An introduction to information theory symbols, signals and noise. Dover Publications.\n","wordCount":"495","inLanguage":"en","datePublished":"2022-09-02T22:13:31+03:00","dateModified":"2022-09-02T22:13:31+03:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://altayatalayy.github.io/Notes/posts/information_theory/"},"publisher":{"@type":"Organization","name":"Engineering Notes","logo":{"@type":"ImageObject","url":"https://altayatalayy.github.io/Notes/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://altayatalayy.github.io/Notes/ accesskey=h title="Engineering Notes (Alt + H)">Engineering Notes</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://altayatalayy.github.io/Notes/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://altayatalayy.github.io/Notes/>Home</a>&nbsp;»&nbsp;<a href=https://altayatalayy.github.io/Notes/posts/>Posts</a></div><h1 class=post-title>Information Theory</h1><div class=post-meta><span title='2022-09-02 22:13:31 +0300 +0300'>September 2, 2022</span></div></header><div class=post-content><h2 id=the-origins-of-information-theory>The Origins of Information Theory<a hidden class=anchor aria-hidden=true href=#the-origins-of-information-theory>#</a></h2><p>A particular quantity called entropy is used in thermodynamics and in statistical mechanics.
A quantity called entropy is used in communication theory. After all thermodynamics and
statistical mechanics are older than communication theory. From these facts we might conclude
that communication theory somehow grew out of statistical mechanics.</p><p>This easy but misleading idea has caused a great deal of confusion even among technical men.
Actually, communication theory evolved from an effort to solve certain problems in the field of
electrical communication. Its entropy was called by mathematical analogy with the entropy of
statistical mechanics.</p><p>While thermodynamics gave us the concept of entropy, it does not give a detailed picture of entropy,
in terms of positions and velocities of molecules, for instance. Statistical mechanics does give a detailed
mechanical meaning to entropy in particular cases. In general, the meaning is that an increase in entropy
means a decrease in order. Order in the sense in which it is used in statistical mechanics involves unpredictability
based on a lack of knowledge of the positions and velocities of molecules.</p><p>In communication theory we consider a message source, such as a writer or a speaker, which may produce on a given
occasion any one of many possible messages. The amount of information conveyed by the message increases as the amount
of uncertainty as to what message actually will be produced becomes greater. A message which is one out of ten messages
conveys a smaller amount of information than a message which is one out of a million possible messages. The entropy of communication
theory is a measure of this uncertainty and the uncertainty, or entropy is taken as the measure of the amount of information conveyed
by a message source.</p><p>In single-current telegraphy we have two elements out of which to construct our code: current and no current, which we might
call 1 and 0. In double-current telegraphy we really have three elements, which we might characterize as forward current;
no current; backward current; or as +1,0,-1. In 1874 Thomas Edison went further; in his quadruplex telegraph system he
used two intensities of current as well as two directions of current. How much information it is possible to send over
a circuit depends not only on how fast one can send successive symbols over the circuit but also on how many different symbols
one has avaible to choose among.</p><p>Then use of multiplicity of symbols can lead to difficulties. Dots and dashes sent over a long cable tend to spread out and
overlap. Thus, when we look for one symbol at the far end we see a little of several others.</p><p>Extraneous currents, which we call noise are always present to interfere with signals sent. Thus, even if we avoid the overlapping
of dots and spaces which is called symbol interference, noise tends to distort received signal.</p><h2 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h2><p>[1] Pierce, J. R. (2012). An introduction to information theory symbols, signals and noise. Dover Publications.</p></div><footer class=post-footer><ul class=post-tags></ul><nav class=paginav><a class=next href=https://altayatalayy.github.io/Notes/posts/simulation/collision/><span class=title>Next »</span><br><span>Collision</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2022 <a href=https://altayatalayy.github.io/Notes/>Engineering Notes</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>