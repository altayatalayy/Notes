<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Posts on Engineering Notes</title><link>https://altayatalayy.github.io/Notes/posts/</link><description>Recent content in Posts on Engineering Notes</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Fri, 02 Sep 2022 22:13:31 +0300</lastBuildDate><atom:link href="https://altayatalayy.github.io/Notes/posts/index.xml" rel="self" type="application/rss+xml"/><item><title>Information Theory</title><link>https://altayatalayy.github.io/Notes/posts/information_theory/</link><pubDate>Fri, 02 Sep 2022 22:13:31 +0300</pubDate><guid>https://altayatalayy.github.io/Notes/posts/information_theory/</guid><description>The Origins of Information Theory A particular quantity called entropy is used in thermodynamics and in statistical mechanics. A quantity called entropy is used in communication theory. After all thermodynamics and statistical mechanics are older than communication theory. From these facts we might conclude that communication theory somehow grew out of statistical mechanics.
This easy but misleading idea has caused a great deal of confusion even among technical men. Actually, communication theory evolved from an effort to solve certain problems in the field of electrical communication.</description></item></channel></rss>