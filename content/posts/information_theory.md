---
title: "Information Theory"
date: 2022-09-02T22:13:31+03:00
draft: false
---
## The Origins of Information Theory

A particular quantity called entropy is used in thermodynamics and in statistical mechanics.
A quantity called entropy is used in communication theory. After all thermodynamics and
statistical mechanics are older than communication theory. From these facts we might conclude
that communication theory somehow grew out of statistical mechanics. 

This easy but misleading idea has caused a great deal of confusion even among technical men.
Actually, communication theory evolved from an effort to solve certain problems in the field of
electrical communication. Its entropy was called by mathematical analogy with the entropy of
statistical mechanics.

While thermodynamics gave us the concept of entropy, it does not give a detailed picture of entropy,
in terms of positions and velocities of molecules, for instance. Statistical mechanics does give a detailed
mechanical meaning to entropy in particular cases. In general, the meaning is that an increase in entropy
means a decrease in order. Order in the sense in which it is used in statistical mechanics involves unpredictability
based on a lack of knowledge of the positions and velocities of molecules.

In communication theory we consider a message source, such as a writer or a speaker, which may produce on a given
occasion any one of many possible messages. The amount of information conveyed by the message increases as the amount
of uncertainty as to what message actually will be produced becomes greater. A message which is one out of ten messages
conveys a smaller amount of information than a message which is one out of a million possible messages. The entropy of communication
theory is a measure of this uncertainty and the uncertainty, or entropy is taken as the measure of the amount of information conveyed
by a message source.

In single-current telegraphy we have two elements out of which to construct our code: current and no current, which we might
call 1 and 0. In double-current telegraphy we really have three elements, which we might characterize as forward current;
no current; backward current; or as +1,0,-1. In 1874 Thomas Edison went further; in his quadruplex telegraph system he
used two intensities of current as well as two directions of current. How much information it is possible to send over
a circuit depends not only on how fast one can send successive symbols over the circuit but also on how many different symbols
one has avaible to choose among.

Then use of multiplicity of symbols can lead to difficulties. Dots and dashes sent over a long cable tend to spread out and
overlap. Thus, when we look for one symbol at the far end we see a little of several others.

Extraneous currents, which we call noise are always present to interfere with signals sent. Thus, even if we avoid the overlapping
of dots and spaces which is called symbol interference, noise tends to distort received signal.


## References
[1] Pierce, J. R. (2012). An introduction to information theory symbols, signals and noise. Dover Publications.
